{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm54fs/yM3l/pihzdWoUGh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinijalanda93/LLM_Assignments/blob/main/srinija_lab1_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer as Atencoder #this is used to token the sentences based on the model simple words->number(tensor format) like encoding process\n",
        "from transformers import AutoModelForCausalLM as Atmodel #it is nn model llm generate for prediction of next word"
      ],
      "metadata": {
        "id": "E4DOpkGvgJv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rChtOzl7P-2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Implement a descriptive Question answering System (Like\n",
        "ChatGPT/Gemini) using NLG by utilizing the LLM Models (any\n",
        "foundation model can use).\n",
        "### b. While testing the Model, Understand the concept of Prompt\n",
        "Engineering, Optimization of the Prompt and Understand the impact\n",
        "of prompt formulation on model output. (Give all the insights as an\n",
        "example in the Colab file)\n",
        "### c. Enter any 3 the same prompt in your model, Gemini and ChatGPT and\n",
        "retrieve the answer and upload. Understand the difference of prompt\n",
        "in each model (as shown the following Gemini and ChatGPT).\n",
        "### d. Upload any 3 the sample question and answers that you have\n",
        "generated (in Colab file)."
      ],
      "metadata": {
        "id": "IqItGEDG5pI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name={\n",
        "    'gpt2':'gpt2', #larger version\n",
        "    'distilgpt2':'distilgpt2', #smaller version\n",
        "    'llma':'meta-llama/Llama-2-7b-chat-hf',# containes 345parameters\n",
        "    'gpt2_large':'gpt2-large', #larger paarameter higher\n",
        "    'gpt2_xl':'gpt2-xl' ,#billion parameter\n",
        "    'falcon':'tiiuae/falcon-7b-instruct'\n",
        "}"
      ],
      "metadata": {
        "id": "_Rx-yHcDqoag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_model=int(input(\"enter the model number:\"))\n",
        "if select_model==1:\n",
        "  models=model_name['gpt2']\n",
        "  print(\"the select name is\",models)\n",
        "elif select_model==2:\n",
        "  models=model_name['distilgpt2']\n",
        "elif select_model==3:\n",
        "  models=model_name['llma']\n",
        "elif select_model==4:\n",
        "  models=model_name['gpt2_large']\n",
        "elif select_model==5:\n",
        "  models=model_name['gpt2_xl']\n",
        "else:\n",
        "  models=model_name['falcon']"
      ],
      "metadata": {
        "id": "ouu0wO-wQM39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48da7563-9a51-4b5b-e006-bc4f17616131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the model number:1\n",
            "the select name is gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=input(\"Enter your question:\")\n",
        "print(\"The prompt is :\",prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i49xTjLrln_",
        "outputId": "93fdedfc-3def-4279-b49f-d9536e344555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question:christ university\n",
            "The prompt is : christ university\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Atencoder.from_pretrained(models)\n",
        "model = Atmodel.from_pretrained(models)\n",
        "\n",
        "print(\"The Tokenizer:\",tokenizer)\n",
        "print(\"the model is:\",model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890,
          "referenced_widgets": [
            "9990003b86d942bbb9f8da6fc1251f8f",
            "84170ae9d632486a8cbbcd023b519e8e",
            "8f01cf5e24ef4150a09170de6459422c",
            "d6ddb5f1ee814df49d4bb2dd277bc776",
            "23319d3599494dda874ca029db7e8150",
            "5f61fc847ce64c3592fb4ad99dca676a",
            "e0bb097556d84d5ab841c8fc42cfdfa9",
            "8cb5ad164c4149cba5ea3ab68118cdaf",
            "6132a1b36d014695bde6e5df71de7096",
            "90eb21d8f5c44011ae4d4b593ce65eb9",
            "8c325c08855a4722a24d085609273dde",
            "67eaa5bab61a47979b5c8778ae8b9abe",
            "4c54cc6d48ad4f50bb54a561423c2886",
            "486617e999f34c8995ab6f633c428e5c",
            "b964d8e0c55a4bf6bc6d28076dbc618d",
            "30a68230e69342469de1cbd3e491c3fe",
            "b755d7c55ac54d4887a38ad02d862a5d",
            "67f6b894b1744e7181b249e0b67b97b1",
            "cf41ed20642048b695c7528f2bcd11ec",
            "1cfe1e459e344944a3ba1c9aae2c4f70",
            "baf2d228235249e98a005ab370591be0",
            "0774e8483bfc4bedb90be1bae3bee6d9",
            "9432c3ad1728400eb80fb344a5cc9637",
            "9f3c0af953a34031a3e03be221e8be61",
            "9e9e72735b154d58948a7fc9e72392a0",
            "2eb3c55f91ea44caba458fb375aa7d7b",
            "f4a2f78bb3c6458eab515eed461a667e",
            "7556a6de4a264b809b65b0766c4ce9ba",
            "e5c30d49d2c5439f9df7191ea4e5b47a",
            "24f0878088704e8cb0e33b2a3b5575ca",
            "16aae5bf8d494e0ab39d13ccbcc8a1a3",
            "7ab4afff9fed473894ad70806292799a",
            "bc5bc9e7c12e425d89ba505d3e15ce2e",
            "8cc1343d2c764c19a655cb0a98581d43",
            "a16518ca6241436ab9acecce993bfb5c",
            "bb34b3cdc5ef41cc85b0d7935d36d23a",
            "83808a63e1c0486f8067c3138805dda1",
            "b681ef31f9e24f788471541fdb6909dd",
            "e4f69068fd5248b79038cb0bed20de86",
            "e70467f0db1c49169e6b86352ce7d9b7",
            "5d3bf8f6dca14a20980a044cba033e27",
            "61b3142e26754cfdad4cb740be61c439",
            "f87541fe673f46e3ac535edb8be72789",
            "5059ac467279466c8675f79ee7368f9b",
            "36e4e6c11f784dd2b2e650b2a2ac71de",
            "ab8cf63d0e8c4d9fb9795ed2782c6093",
            "d6c9cfd43eed45019bba2cb24ec69334",
            "1e408003cf774ac4b437afe4d4ff5139",
            "3a5930511ce04a769e2090ce64568f12",
            "a7935751f5fd469bb33b911f759a2f58",
            "d461bcf6dbcc459ead6a83d870a58224",
            "53990e84427e4374bb09037baba951d2",
            "e930ba638d8047c6bcd7a0cf2c90a12a",
            "d8a496fab20841abac85ccf1743473c3",
            "57f2b11bfcee44dcb2f9504a445b0c87",
            "b836f81bd6824504a0af25f43b7389dd",
            "5666d6b519cd4bcabfbcd1c82e05ca31",
            "b45d121f01b941869cca09fccbe5b3ed",
            "cb5e4f567e5d4f648585a609920890c5",
            "302d3d30805d4e5aa5ca0484cf17d50d",
            "5b64e43146ae408884674599217e3610",
            "d66e1389545748479dbe00f521cd7c52",
            "963e30a99258474fa9e1739ff4f72be3",
            "301e67dd757b4c9ba57006804be25da2",
            "49cfa951ed3747c48fc38b726c634517",
            "74aea01410264dc7b65ba5227f3ea568",
            "23fe5e7ac501410da95ee5c0a852c2ec",
            "c81cefd2220b492dac15bfa0408881d9",
            "e77beb2cc9e94f6c8ec378ac09272006",
            "3fdef08f7f5848a5864bfa54ccda5cb0",
            "b7832dc8b9e545e1a8c6f0ea61336b59",
            "d50adfe09bf346dfabdf056fde4e1d56",
            "f04668a37d2948dd884edb7d03bbae43",
            "bf0382c5884f472989da2d95566cb57b",
            "9883d669539a4c2e97387873f10a735a",
            "002232447e0a4f959f18c24d48aac315",
            "2c6cf59da56048378d5148a33f098689"
          ]
        },
        "id": "6F2Hvt7igC1d",
        "outputId": "143ecf87-6faa-4fde-b4fa-33e9c4e8ca60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9990003b86d942bbb9f8da6fc1251f8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67eaa5bab61a47979b5c8778ae8b9abe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9432c3ad1728400eb80fb344a5cc9637"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cc1343d2c764c19a655cb0a98581d43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36e4e6c11f784dd2b2e650b2a2ac71de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b836f81bd6824504a0af25f43b7389dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23fe5e7ac501410da95ee5c0a852c2ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Tokenizer: GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "}\n",
            ")\n",
            "the model is: GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def output(input,maxLength=200,temperature=0.7):\n",
        "  answer=tokenizer(input,return_tensors=\"pt\")\n",
        "  print(\"---------------------------------------\")\n",
        "  print(\"the contains in the tokenzier are:\",answer)\n",
        "\n",
        "  print(\"==========================================\")\n",
        "  outputs=model.generate(answer.input_ids, #tokens ids\n",
        "                         attention_mask=answer.attention_mask, # used to determine which need to focus on tokens\n",
        "        max_length=maxLength, #length of the output\n",
        "        temperature=temperature, #descibe about the randomness HT=HR LT=Hprediction\n",
        "        num_return_sequences=1, # no of answer to product\n",
        "        do_sample=True, #sample next word in boolen\n",
        "        top_k=50, #top sample token to consider likely tokens\n",
        "        top_p=0.95,# probability distribution on next token\n",
        "        pad_token_id=tokenizer.pad_token_id)\n",
        "  print(\"=======================================\")\n",
        "  print(\"the output is:\",outputs)\n",
        "  print(\"=======================================\")\n",
        "  response=tokenizer.decode(outputs[0])\n",
        "  return response"
      ],
      "metadata": {
        "id": "A6Q0I7Slhqqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=output(prompt)\n",
        "print(\"the response is :\",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi00op0UkKE2",
        "outputId": "fdd7de51-4101-4b6a-a635-1401400cce8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "the contains in the tokenzier are: {'input_ids': tensor([[10919,  1148,  9552,    30]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
            "==========================================\n",
            "=======================================\n",
            "the output is: tensor([[10919,  1148,  9552,    30,   198,   198, 20185,   318,   281, 11666,\n",
            "          4430,  1912,   319,  4572,  4673,    13, 35941,  4430,   318,   257,\n",
            "          1296,   286, 29964,   326,   460,   307,  1760,   287,   262,  1103,\n",
            "           995,    13,  9552,   318,   257,  1296,   286, 29964,   326,   460,\n",
            "           307,  1760,   287,   262,  1103,   995,    13,   198,   198,  2514,\n",
            "          1833,  9552,    11,   356,   761,   284,  1833,   644,   318,  5836,\n",
            "           287,   262,  1103,   995,    13,   198,   198,   464,  1306,  2665,\n",
            "         19451,   703,  9552,   318,   973,   287,   262,  1103,   995,    13,\n",
            "           198,   198,  2437,  9552,   318,   973,   287,   262,  1103,   995,\n",
            "           198,   198,   464,  1708,  9004,  2148,   281, 16700,   286,   703,\n",
            "          9552,   318,   973,   287,   262,  1103,   995,    13,   198,   198,\n",
            "          2437,  9552,   318,   973,   287,   262,  1103,   995,   198,   198,\n",
            "           464,  1708,  9004,  2148,   281, 16700,   286,   703,  9552,   318,\n",
            "           973,   287,   262,  1103,   995,    13,   198,   198,  2437,  9552,\n",
            "           318,   973,   287,   262,  1103,   995,   198,   198,   464,  1708,\n",
            "          9004,  2148,   281, 16700,   286,   703,  9552,   318,   973,   287,\n",
            "           262,  1103,   995,    13,   198,   198,  2437,  9552,   318,   973,\n",
            "           287,   262,  1103,   995,   198,   198,   464,  1708,  9004,  2148,\n",
            "           281, 16700,   286,   703,  9552,   318,   973,   287,   262,  1103,\n",
            "           995,    13,   198,   198,  2437,  9552,   318,   973,   287,   262,\n",
            "          1103,   995,   198,   198,   464,  1708,  9004,  2148,   281, 16700,\n",
            "           286,   703,  9552,   318,   973,   287,   262,  1103,   995,    13,\n",
            "           198,   198,  2437,  9552,   318,   973,   287,   262,  1103,   995,\n",
            "           198,   198,   464,  1708,  9004,  2148,   281, 16700,   286,   703,\n",
            "          9552,   318,   973,   287,   262,  1103,   995,    13,   198,   198,\n",
            "          2437,  9552,   318,   973,   287,   262,  1103,   995,   198,   198,\n",
            "           464,  1708,  9004,  2148,   281, 16700,   286,   703,  9552,   318,\n",
            "           973,   287,   262,  1103,   995,    13,   198,   198,  2437,  9552,\n",
            "           318,   973,   287,   262,  1103,   995,   198,   198,   464,  1708,\n",
            "          9004,  2148,   281, 16700,   286,   703,  9552,   318,   973,   287,\n",
            "           262,  1103,   995,    13,   198,   198,  2437,  9552,   318,   973,\n",
            "           287,   262,  1103,   995,   198,   198,   464,  1708,  9004,  2148]])\n",
            "=======================================\n",
            "the response is : what Is AI?\n",
            "\n",
            "AI is an artificial intelligence based on machine learning. Artificial intelligence is a form of computation that can be done in the real world. AI is a form of computation that can be done in the real world.\n",
            "\n",
            "To understand AI, we need to understand what is happening in the real world.\n",
            "\n",
            "The next section discusses how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide an overview of how AI is used in the real world.\n",
            "\n",
            "How AI is used in the real world\n",
            "\n",
            "The following sections provide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractive QA\n",
        "### which are NLP task in which model identifies and extarcts the answer  to the questions directly from text that we defined!! mean it doesn't contain any synthesis or providing any new text what ever predefined content is there it will provide that only..\n",
        "### EXAMPLE:BERT,DistilBERT,RoBERTa\n",
        "\n",
        "## Generative QA\n",
        "### which tried making synthesis /own words answer\n",
        "### EXAMPLE:T5, FLAN-T5, GPT, LLaMA, Falcon"
      ],
      "metadata": {
        "id": "tYMnIDLNSUkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_response(prompt, max_length=200, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,  # Add attention mask\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.pad_token_id  # Explicitly set pad token ID\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Part a: Descriptive QA System\n",
        "def qa_system(question, context=\"\"):\n",
        "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer in a detailed and descriptive manner:\\n\"\n",
        "    response = generate_response(prompt)\n",
        "    return response\n",
        "\n",
        "# Part b: Prompt Engineering Demonstration\n",
        "def demonstrate_prompt_engineering():\n",
        "    question = \"Explain how photosynthesis works\"\n",
        "\n",
        "    # Different prompt formulations\n",
        "    prompts = [\n",
        "        # Basic prompt\n",
        "        f\"Question: {question}\\nAnswer briefly:\",\n",
        "        # Detailed prompt with context\n",
        "        f\"Question: {question}\\nContext: Photosynthesis is a process used by plants.\\nAnswer in detail with scientific terminology:\",\n",
        "        # Guided prompt\n",
        "        f\"Question: {question}\\nPlease explain step-by-step, including inputs, outputs, and location in the cell:\"\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        response = generate_response(prompt)\n",
        "        results.append({\n",
        "            \"Prompt_Type\": f\"Prompt {i}\",\n",
        "            \"Prompt\": prompt,\n",
        "            \"Response\": response\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\nPrompt Engineering Results:\")\n",
        "    print(df[[\"Prompt_Type\", \"Response\"]])\n",
        "\n",
        "    print(\"\\nPrompt Engineering Insights:\")\n",
        "    print(\"1. Basic prompt yields shorter, simpler responses.\")\n",
        "    print(\"2. Contextual prompt provides more scientifically accurate responses.\")\n",
        "    print(\"3. Guided prompt results in structured, step-by-step explanations.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def compare_models():\n",
        "    question = \"What is the capital city of France?\"\n",
        "    prompt = f\"Question: {question}\\nAnswer concisely:\"\n",
        "\n",
        "    # Our model's response\n",
        "    our_response = generate_response(prompt)\n",
        "\n",
        "    # Simulated Gemini and ChatGPT responses (since we can't access them directly)\n",
        "    gemini_response = \"The capital city of France is Paris.\"\n",
        "    chatgpt_response = \"Paris is the capital city of France.\"\n",
        "\n",
        "    comparison = {\n",
        "        \"Model\": [\"Our Model (DistilGPT-2)\", \"Gemini\", \"ChatGPT\"],\n",
        "        \"Response\": [our_response, gemini_response, chatgpt_response]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(comparison)\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nModel Comparison Insights:\")\n",
        "    print(\"1. Our model may generate slightly longer responses due to generative nature.\")\n",
        "    print(\"2. Gemini tends to be very concise and direct.\")\n",
        "    print(\"3. ChatGPT provides balanced, natural responses.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Part d: Sample Questions and Answers\n",
        "def generate_sample_qa():\n",
        "    questions = [\n",
        "        \"What is the theory of relativity?\",\n",
        "        \"How does a computer process information?\",\n",
        "        \"What causes climate change?\"\n",
        "    ]\n",
        "\n",
        "    sample_qa = []\n",
        "    for q in questions:\n",
        "        response = qa_system(q)\n",
        "        sample_qa.append({\n",
        "            \"Question\": q,\n",
        "            \"Answer\": response\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(sample_qa)\n",
        "    print(\"\\nSample Questions and Answers:\")\n",
        "    print(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    print(\"=== MAI 475 Lab Exercise 1 ===\")\n",
        "\n",
        "    print(\"\\nTesting QA System:\")\n",
        "    test_question = \"What is artificial intelligence?\"\n",
        "    print(f\"Question: {test_question}\")\n",
        "    print(f\"Answer: {qa_system(test_question)}\")\n",
        "\n",
        "    # Part b: Prompt Engineering\n",
        "    prompt_df = demonstrate_prompt_engineering()\n",
        "\n",
        "    # Part c: Model Comparison\n",
        "    compare_df = compare_models()\n",
        "\n",
        "    sample_df = generate_sample_qa()\n",
        "\n",
        "    prompt_df.to_csv(\"prompt_engineering_results.csv\", index=False)\n",
        "    compare_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
        "    sample_df.to_csv(\"sample_qa_results.csv\", index=False)\n",
        "\n",
        "    print(\"\\nResults saved to CSV files.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d57ad91c5db94195b9163f771d71f47c",
            "48bf9d55c44b4ddca5a27cbc001c7f59",
            "c4368072ec894e7a8f35f9343e39f1be",
            "167518b3afae4ab8825d4c6d0b2f5b78",
            "f34f6b8ec8e94f0eba02378f0df96fac",
            "008c5200a3c447a08c7505b44ceee87c",
            "100cddee0c7748afbdd2dcbd5cf7ed43",
            "e7a9b8fad9af4178ada211b87ca7fff9",
            "71c1bdfb0e2043fcb3e182f8c0bd3a49",
            "18e5c82cabeb4261af4eeee4e3c946c4",
            "d2cec36c9a214596a995cf6498417f49",
            "bd63abc2b79f4767967abda85f1445cc",
            "bce0d0f87b114343b5d3bc3933cd5ebe",
            "2bed984e5b2141c885d44d4b67b4e5fc",
            "c4e4486caf9d4dc8b195f99527caf1c1",
            "f0bde3c4d7a6497bad72d88e05f7e934",
            "f7b05df61b454fd89cf2d71eae1a9e45",
            "2347b580e9bb401fbcc6f9aa9eef62cc",
            "6a79bfbe3cca41a197c72ce19b51e7ef",
            "415254e8f55749098d80c984d2ef5d79",
            "7c587b5ddd1a4c4bbe6e56b1021d617e",
            "6463b1bd30824a18aba297cce5a3acdf",
            "8c0756994c1f43988e2d67921a86618c",
            "4f5cdd4495164b34bfc50293b259fe1a",
            "952659c9416d477987751bd1ad324900",
            "2bf2109b730d44bc8845899de9661467",
            "c7d0986bd84e488bbdd48a0116995e8e",
            "8e912dd09fa64d8182991477cd0ac735",
            "5a4834d6420e4acf95eee4cfc61fc7fd",
            "5fadf450557b47feb378398775b59883",
            "9bf74e579528424c92212b78fa988c57",
            "4cab1e70906a4e1dbdfa3213ef0c6166",
            "7fdceefbbd004a76ba7e21536195c8fb",
            "66b1d3a7e99a43c79bacaa118bec1b3c",
            "321ce5f925884bbd9ab0a00bf10e4c44",
            "e4d2d2c105fe4b54af068595dd7928be",
            "dadd1ccda55b488da1bd5f387744bb4a",
            "f92deb95c9e0478ebf064d143658d824",
            "f0668c7d06644b4aaef0db718f42a468",
            "2661df452f3b4885b4f07a36abec7301",
            "45d54382bbc14be493497c0dd99d4184",
            "2095affa92f0471eafb22c10ecb95a3d",
            "2b7de3160b2b49859d20274a0a612f21",
            "52ed6b7ae0564275b9b79d3998e7b938",
            "11f64ba993204b9fb73a07bc13e12b36",
            "8f78a1a9678b4269a644956da12376ad",
            "eff770aed71c4417b4e1d633d6f15d50",
            "5cb18fddb5a848d2a65a53cef7ffc5d9",
            "1f7a7df57a2949b6a1242a076f8cb340",
            "395969dceb1e4cf6bf59a637bd85425b",
            "a85d439b63f540468575b2b6001d8ff8",
            "4590e83bc09543009769baf6d7d274ac",
            "32dc015dccd94dd78988bb95a93f68ec",
            "1e678932e57c4a31b9df6ff663b71c0f",
            "fe78b6d0f6634c8ba2f9abee14c306b1",
            "f66d1efaad9a4d78acafce312cd2d812",
            "8ae14ac402e646bcb8f228e21dec751a",
            "7508f2019a734d3ca258d0ff049b8cfe",
            "b6a7e0b7ffac4fd7a41285a3d6fa0cc2",
            "031ed0156b664d399561be17cd1de37b",
            "ce522cd50ea44543a714084d4acbd190",
            "54d2689af01a45cdb190ab2789c63e08",
            "6ba3d994f8954ce59ae9277a866f94cb",
            "fc6b8494e9db4cf7bac4e8cd4c02642a",
            "05932e40a4704b4e904a5e10f5d57007",
            "30aa368c8b554799a39d3ee6f04c8ef6",
            "13e5d7e99c3f4da6b2971335588c41ab",
            "6d5cdc84b3dc47a9b4f27b7fc66f26a4",
            "6feabe06b1a849aea3fbee0f781d9a56",
            "3e80df00d3d043ee91e6ba8d3d1f8381",
            "b359e148ae574a499be4a7ac6b42d797",
            "ba2a33b588ea44218c425bf14759d8a2",
            "cb631aa4811b4a38bc5f5d57eafae09b",
            "2a832f773e984f50bfb716ad6cd22d6b",
            "1a0a91613bd74d6c805251b49633e11e",
            "7538a770378442c490ad6a0bfe17ce9a",
            "0e1a384d1a244bdca4fa8df9132ece70"
          ]
        },
        "id": "YBfn_GbOAEpi",
        "outputId": "c5982e85-35a6-4e43-888b-d0516bb39c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d57ad91c5db94195b9163f771d71f47c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd63abc2b79f4767967abda85f1445cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c0756994c1f43988e2d67921a86618c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66b1d3a7e99a43c79bacaa118bec1b3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11f64ba993204b9fb73a07bc13e12b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f66d1efaad9a4d78acafce312cd2d812"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13e5d7e99c3f4da6b2971335588c41ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MAI 475 Lab Exercise 1 ===\n",
            "\n",
            "Testing QA System:\n",
            "Question: What is artificial intelligence?\n",
            "Answer: Question: What is artificial intelligence?\n",
            "Context: \n",
            "Answer in a detailed and descriptive manner:\n",
            "I have not seen any artificial intelligence in the past or in the past. It seems that I am not aware of it, but I have seen it in the past, and I have seen it in the past. It seems that I am not aware of it, but I have seen it in the past.\n",
            "This is not a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about the nature of artificial intelligence. It is a question about\n",
            "\n",
            "Prompt Engineering Results:\n",
            "  Prompt_Type                                           Response\n",
            "0    Prompt 1  Question: Explain how photosynthesis works\\nAn...\n",
            "1    Prompt 2  Question: Explain how photosynthesis works\\nCo...\n",
            "2    Prompt 3  Question: Explain how photosynthesis works\\nPl...\n",
            "\n",
            "Prompt Engineering Insights:\n",
            "1. Basic prompt yields shorter, simpler responses.\n",
            "2. Contextual prompt provides more scientifically accurate responses.\n",
            "3. Guided prompt results in structured, step-by-step explanations.\n",
            "\n",
            "Model Comparison:\n",
            "                     Model                                           Response\n",
            "0  Our Model (DistilGPT-2)  Question: What is the capital city of France?\\...\n",
            "1                   Gemini               The capital city of France is Paris.\n",
            "2                  ChatGPT               Paris is the capital city of France.\n",
            "\n",
            "Model Comparison Insights:\n",
            "1. Our model may generate slightly longer responses due to generative nature.\n",
            "2. Gemini tends to be very concise and direct.\n",
            "3. ChatGPT provides balanced, natural responses.\n",
            "\n",
            "Sample Questions and Answers:\n",
            "                                   Question  \\\n",
            "0         What is the theory of relativity?   \n",
            "1  How does a computer process information?   \n",
            "2               What causes climate change?   \n",
            "\n",
            "                                              Answer  \n",
            "0  Question: What is the theory of relativity?\\nC...  \n",
            "1  Question: How does a computer process informat...  \n",
            "2  Question: What causes climate change?\\nContext...  \n",
            "\n",
            "Results saved to CSV files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUiK9sK_B7Uu",
        "outputId": "0769d71c-7db3-43ba-a6b3-de65e093a9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key=\"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\")\n"
      ],
      "metadata": {
        "id": "t0kwgmwfwj_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_llm(prompt: str, model=\"meta-llama/llama-4-scout-17b-16e-instruct\"):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=1,\n",
        "        max_completion_tokens=1024,\n",
        "        top_p=1,\n",
        "        stream=True,\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in completion:\n",
        "        response += chunk.choices[0].delta.content or \"\"\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "uevfMYnwwtaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=input(\"Enter your queries:\")\n",
        "\n",
        "print(\"the output is :\",ask_llm(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjcd3q59w3nJ",
        "outputId": "c0ad60dc-5f16-41de-eedc-cbb0b736428b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your queries:describe about christ university?\n",
            "the output is : Christ University, also known as Christ College, is a private university located in Bangalore, Karnataka, India. Here are some key features and facts about Christ University:\n",
            "\n",
            "**History**: Christ University was founded in 1969 as Christ College by Fr. A. K. Mammen. It was granted autonomy in 1990 and was affiliated to Bangalore University. In 2003, it was recognized as a \"Deemed to be University\" by the University Grants Commission (UGC) and was renamed Christ University.\n",
            "\n",
            "**Academics**: Christ University offers undergraduate, postgraduate, and doctoral programs across various disciplines, including:\n",
            "\n",
            "* Arts, Humanities, and Social Sciences\n",
            "* Commerce and Economics\n",
            "* Science\n",
            "* Engineering and Technology\n",
            "* Law\n",
            "* Management\n",
            "* Education\n",
            "\n",
            "The university is known for its strong programs in business, commerce, and engineering.\n",
            "\n",
            "**Facilities**: The university campus is spread over 25 acres and features:\n",
            "\n",
            "* Well-equipped classrooms and laboratories\n",
            "* Library with a vast collection of books, journals, and online resources\n",
            "* Sports facilities, including a swimming pool, gymnasium, and playgrounds\n",
            "* Hostel facilities for students\n",
            "\n",
            "**Research and Innovation**: Christ University has a strong focus on research and innovation. The university has established research centers and collaborations with industry partners to promote research and development.\n",
            "\n",
            "**Accreditations and Rankings**: Christ University has received several accreditations and rankings, including:\n",
            "\n",
            "* NAAC (National Assessment and Accreditation Council) accreditation with an 'A' grade\n",
            "* Ranked among the top 100 universities in India by NIRF (National Institutional Ranking Framework)\n",
            "* Ranked among the top 50 private universities in India by India Today\n",
            "\n",
            "**Placements**: Christ University has a strong placement record, with many top companies visiting the campus for recruitment. The university's placement cell provides training and support to students to help them secure jobs.\n",
            "\n",
            "**International Collaborations**: Christ University has collaborations with several international universities, including:\n",
            "\n",
            "* University of Greenwich, UK\n",
            "* University of Canberra, Australia\n",
            "* University of Wisconsin, USA\n",
            "\n",
            "These collaborations provide opportunities for student exchange programs, research collaborations, and joint academic programs.\n",
            "\n",
            "**Notable Alumni**: Christ University has a strong alumni network, with many notable alumni in various fields, including:\n",
            "\n",
            "* Business and industry leaders\n",
            "* Politicians and civil servants\n",
            "* Artists and media professionals\n",
            "* Sports personalities\n",
            "\n",
            "Overall, Christ University is a well-established and reputable institution in India, known for its strong academic programs, research focus, and international collaborations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uby6mVb4AXPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjrSQ08AB4NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uk36udvPlSnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}